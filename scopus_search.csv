,eid,doi,pii,pubmed_id,title,subtype,subtypeDescription,creator,afid,affilname,affiliation_city,affiliation_country,author_count,author_names,author_ids,author_afids,coverDate,coverDisplayDate,publicationName,issn,source_id,eIssn,aggregationType,volume,issueIdentifier,article_number,pageRange,description,authkeywords,citedby_count,openaccess,freetoread,freetoreadLabel,fund_acr,fund_no,fund_sponsor
0,2-s2.0-85170391256,,,,Towards Gender Fairness for Mental Health Prediction,cp,Conference Paper,Cheong J.,60111768;60031101;60004305,The Alan Turing Institute;University of Cambridge;Middle East Technical University (METU),London;Cambridge;Ankara,United Kingdom;United Kingdom;Turkey,4,"Cheong, Jiaee;Kuzucu, Selim;Kalkan, Sinan;Gunes, Hatice",57324430800;57994552400;58600778700;7005067251,60031101-60111768;60004305;60004305;60031101,2023-01-01,2023,IJCAI International Joint Conference on Artificial Intelligence,10450823,19400157504,,Conference Proceeding,2023-August,,,5932-5940,"Mental health is becoming an increasingly prominent health challenge. Despite a plethora of studies analysing and mitigating bias for a variety of tasks such as face recognition and credit scoring, research on machine learning (ML) fairness for mental health has been sparse to date. In this work, we focus on gender bias in mental health and make the following contributions. First, we examine whether bias exists in existing mental health datasets and algorithms. Our experiments were conducted using Depresjon, Psykose and D-Vlog. We identify that both data and algorithmic bias exist. Second, we analyse strategies that can be deployed at the pre-processing, in-processing and post-processing stages to mitigate for bias and evaluate their effectiveness. Third, we investigate factors that impact the efficacy of existing bias mitigation strategies and outline recommendations to achieve greater gender fairness for mental health. Upon obtaining counter-intuitive results on D-Vlog dataset, we undertake further experiments and analyses, and provide practical suggestions to avoid hampering bias mitigation efforts in ML for mental health.",,0,0,,,ATI,EP/R030782/1,Alan Turing Institute
1,2-s2.0-85156259419,10.1007/s11135-023-01673-0,,,"Examining the research taxonomy of artificial intelligence, deep learning &amp; machine learning in the financial sphere—a bibliometric analysis",ar,Article,Biju A.K.V.N.,60031566,University of Kerala,Thiruvananthapuram,India,3,"Biju, Ajitha Kumari Vijayappan Nair;Thomas, Ann Susan;Thasneem, J.",57340237400;58222265600;58223018900,60031566;60031566;60031566,2023-01-01,2023,Quality and Quantity,00335177,19372,15737845,Journal,,,,,"This paper surveys the extant literature on machine learning, artificial intelligence, and deep learning mechanisms within the financial sphere using bibliometric methods. We considered the conceptual and social structure of publications in ML, AI, and DL in finance to better understand the research’s status, development, and growth. The study finds an upsurge in publication trends within this research arena, with a bit of concentration around the financial domain. The institutional contributions from USA and China constitute much of the literature on applying ML and AI in finance. Our analysis identifies emerging research themes, with the most futuristic being ESG scoring using ML and AI. However, we find there is a lack of empirical academic research with a critical appraisal of these algorithmic-based advanced automated financial technologies. There are severe pitfalls in the prediction process using ML and AI due to algorithmic biases, mostly in the areas of insurance, credit scoring and mortgages. Thus, this study indicates the next evolution of ML and DL archetypes in the economic sphere and the need for a strategic turnaround in academics regarding these forces of disruption and innovation that are shaping the future of finance.",Artificial intelligence | Bibliometric analysis | Conceptual structure | Deep learning | Machine learning | Social structure,0,1,repositoryvor,Green,,undefined,
2,2-s2.0-85109424638,10.1016/j.ejor.2021.06.023,S0377221721005385,,"Fairness in credit scoring: Assessment, implementation and profit implications",ar,Article,Kozodoi N.,60000762,Humboldt-Universität zu Berlin,Berlin,Germany,3,"Kozodoi, Nikita;Jacob, Johannes;Lessmann, Stefan",57208154515;57222420713;6506377858,60000762;60000762;60000762,2022-03-16,16 March 2022,European Journal of Operational Research,03772217,22489,,Journal,297,3,,1083-1094,"The rise of algorithmic decision-making has spawned much research on fair machine learning (ML). Financial institutions use ML for building risk scorecards that support a range of credit-related decisions. Yet, the literature on fair ML in credit scoring is scarce. The paper makes three contributions. First, we revisit statistical fairness criteria and examine their adequacy for credit scoring. Second, we catalog algorithmic options for incorporating fairness goals in the ML model development pipeline. Last, we empirically compare different fairness processors in a profit-oriented credit scoring context using real-world data. The empirical results substantiate the evaluation of fairness measures, identify suitable options to implement fair credit scoring, and clarify the profit-fairness trade-off in lending decisions. We find that multiple fairness criteria can be approximately satisfied at once and recommend separation as a proper criterion for measuring the fairness of a scorecard. We also find fair in-processors to deliver a good balance between profit and fairness and show that algorithmic discrimination can be reduced to a reasonable level at a relatively low cost. The codes corresponding to the paper are available on GitHub.",Algorithmic fairness | Credit scoring | Machine learning | OR in banking,26,0,repositoryam,Green,,undefined,
3,2-s2.0-85148221981,10.32361/2022140214223,,,THE CHALLENGES TO THE LAW BEFORE THE COMPLEXITIES OF NEW TECHNOLOGIES,ar,Article,De Oliveira Iszlaji B.,129188635,Mestranda pela Pontifícia Universidade Católica de São Paulo. Advogada,Sao Paulo,Portugal,1,"De Oliveira Iszlaji, Bárbara",58104903100,129188635,2022-01-01,2022,Revista de Direito,18068790,21101039229,25270389,Journal,14,2,,,"This article aims to demonstrate how technological and scientific innovation, communication and information have become determinant elements for the development of economic activities, imposing new regulatory and legal challenges. The article was elaborated based on a bibliographic review of different areas of knowledge in the Human and Social Sciences, as well as on Brazilian and on foreign legislation. In recent years, the Brazilian legal framework set forth rules about credit scoring, the uses of internet and the processing of personal data. At the same time, the State and the Judiciary have been constantly urged to solve complex issues related to new technologies, on which they often do not have the necessary knowledge and instruments to deal with effectively. In this context, proceduralization presents itself as a viable model for new technologies, due to its flexibility and learning capacity.",Algorithmic bias | Fake news | Proceduralization | Regulated self-governance | Surveillance capitalism,0,1,repositoryam,Green,,undefined,
4,2-s2.0-85117313496,10.1093/oxrep/grab020,,,Algorithmic fairness in credit scoring,ar,Article,Bono T.,60117020,U.K. Financial Conduct Authority,London,United Kingdom,3,"Bono, Teresa;Croxson, Karen;Giles, Adam",57299632900;54388672600;57299817700,60117020;60117020;60117020,2021-01-01,2021,Oxford Review of Economic Policy,0266903X,22904,14602121,Journal,37,3,,585-617,"The use of machine learning as an input into decision-making is on the rise, owing to its ability to uncover hidden patterns in large data and improve prediction accuracy. Questions have been raised, however, about the potential distributional impacts of these technologies, with one concern being that they may perpetuate or even amplify human biases from the past. Exploiting detailed credit file data for 800,000 UK borrowers, we simulate a switch from a traditional (logit) credit scoring model to ensemble machine-learning methods. We confirm that machine-learning models are more accurate overall. We also find that they do as well as the simpler traditional model on relevant fairness criteria, where these criteria pertain to overall accuracy and error rates for population subgroups defined along protected or sensitive lines (gender, race, health status, and deprivation). We do observe some differences in the way credit-scoring models perform for different subgroups, but these manifest under a traditional modelling approach and switching to machine learning neither exacerbates nor eliminates these issues. The paper discusses some of the mechanical and data factors that may contribute to statistical fairness issues in the context of credit scoring.",ensemble methods | ML fairness | penalized regression | statistical bias,6,0,,,,undefined,
5,2-s2.0-85117196692,10.1109/HPEC49654.2021.9622861,,,Machine Learning Fairness is Computationally Difficult and Algorithmically Unsatisfactorily Solved,cp,Conference Paper,Teodorescu M.H.M.,60031117;60005286,Boston College;Rice University,Chestnut Hill;Houston,United States;United States,2,"Teodorescu, Mike H.M.;Yao, Xinyu",57218569335;57427433300,60031117;60005286,2021-01-01,2021,"2021 IEEE High Performance Extreme Computing Conference, HPEC 2021",,21101073919,,Conference Proceeding,,,,,"The main purpose of the paper is to analyze the computational difficulties of selecting the suitable classification algorithms that satisfy specific ethical criteria, when real data is used in training. Employing an imbalanced credit decision dataset largely used for credit scoring and applying a set of algorithms and several fairness criteria, we show that many typical classification algorithms do not satisfy in a reasonable manner more than one fairness criterion when considering more than one protected attribute. This adds a layer of difficulty to the ones represented by the need of large databases and data-and computationally-intensive decision-making systems as used in domains such as credit scoring and hiring. A novel analysis of this study is directly relating ML/AI fairness criteria and computational complexity. We reframe the problem of complexity by connecting it to the search of an ethically acceptable solution instead of just an accurate solution. The results suggest the continued need for human input in fairness decisions, especially when deciding tradeoffs between fairness criteria.",algorithmic fairness | classification | decision errors | fairness criteria | imbalanced dataset | subgroup fairness,3,0,,,,undefined,
6,2-s2.0-85105164382,,,,Minimax pareto fairness: A multi objective perspective,cp,Conference Paper,Martinez N.,60008724,Duke University,Durham,United States,3,"Martinez, Natalia;Bertran, Martin;Sapiro, Guillermo",57225838627;56429064500;7005450011,60008724;60008724;60008724,2020-01-01,2020,"37th International Conference on Machine Learning, ICML 2020",,21101044400,,Conference Proceeding,PartF168147-9,,,6711-6720,"In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-Time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.",,37,0,,,NSF,R01MH120093,National Science Foundation
